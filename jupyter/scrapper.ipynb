{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4a4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    \"Entrepreneur\",\n",
    "    \"SaaS\",\n",
    "    \"NoStupidQuestions\",\n",
    "    \"personalfinance\",\n",
    "    \"smallbusiness\",\n",
    "    \"socialmedia\",\n",
    "    \"askatherapist\",\n",
    "    \"productivity\",\n",
    "    \"Accounting\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380648c",
   "metadata": {},
   "source": [
    "# Scrapping SubReddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d45e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Union\n",
    "from httpx import AsyncClient, Response\n",
    "from parsel import Selector\n",
    "from loguru import logger as log\n",
    "\n",
    "# initialize an async httpx client\n",
    "client = AsyncClient(\n",
    "    # enable http2\n",
    "    http2=True,\n",
    "    # add basic browser like headers to prevent getting blocked\n",
    "    headers={\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Cookie\": \"intl_splash=false\"\n",
    "    },\n",
    "    follow_redirects=True\n",
    ")\n",
    "\n",
    "def parse_subreddit(response: Response) -> List[Dict]:\n",
    "    \"\"\"parse article data from HTML\"\"\"\n",
    "    selector = Selector(response.text)\n",
    "    url = str(response.url)\n",
    "    info = {}\n",
    "    info[\"id\"] = url.split(\"/r\")[-1].replace(\"/\", \"\")\n",
    "    info[\"description\"] = selector.xpath(\"//shreddit-subreddit-header/@description\").get()\n",
    "    members = selector.xpath(\"//shreddit-subreddit-header/@subscribers\").get()\n",
    "    rank = selector.xpath(\"//strong[@id='position']/*/@number\").get()    \n",
    "    info[\"members\"] = int(members) if members else None\n",
    "    info[\"rank\"] = int(rank) if rank else None\n",
    "    info[\"bookmarks\"] = {}\n",
    "    for item in selector.xpath(\"//div[faceplate-tracker[@source='community_menu']]/faceplate-tracker\"):\n",
    "        name = item.xpath(\".//a/span/span/span/text()\").get()\n",
    "        link = item.xpath(\".//a/@href\").get()\n",
    "        info[\"bookmarks\"][name] = link\n",
    "\n",
    "    info[\"url\"] = url\n",
    "    post_data = []\n",
    "    for box in selector.xpath(\"//article\"):\n",
    "        link = box.xpath(\".//a/@href\").get()\n",
    "        author = box.xpath(\".//shreddit-post/@author\").get()\n",
    "        post_label = box.xpath(\".//faceplate-tracker[@source='post']/a/span/div/text()\").get()\n",
    "        upvotes = box.xpath(\".//shreddit-post/@score\").get()\n",
    "        comment_count = box.xpath(\".//shreddit-post/@comment-count\").get()\n",
    "        attachment_type = box.xpath(\".//shreddit-post/@post-type\").get()\n",
    "        if attachment_type and attachment_type == \"image\":\n",
    "            attachment_link = box.xpath(\".//div[@slot='thumbnail']/*/*/@src\").get()\n",
    "        elif attachment_type == \"video\":\n",
    "            attachment_link = box.xpath(\".//shreddit-player/@preview\").get()\n",
    "        else:\n",
    "            attachment_link = box.xpath(\".//div[@slot='thumbnail']/a/@href\").get()\n",
    "        post_data.append({\n",
    "            \"authorProfile\": \"https://www.reddit.com/user/\" + author if author else None,\n",
    "            \"authorId\": box.xpath(\".//shreddit-post/@author-id\").get(),            \n",
    "            \"title\": box.xpath(\"./@aria-label\").get(),\n",
    "            \"link\": \"https://www.reddit.com\" + link if link else None,\n",
    "            \"publishingDate\": box.xpath(\".//shreddit-post/@created-timestamp\").get(),\n",
    "            \"postId\": box.xpath(\".//shreddit-post/@id\").get(),\n",
    "            \"postLabel\": post_label.strip() if post_label else None,\n",
    "            \"postUpvotes\": int(upvotes) if upvotes else None,\n",
    "            \"commentCount\": int(comment_count) if comment_count else None,\n",
    "            \"attachmentType\": attachment_type,\n",
    "            \"attachmentLink\": attachment_link,\n",
    "        })\n",
    "    # id for the next posts batch\n",
    "    cursor_id = selector.xpath(\"//shreddit-post/@more-posts-cursor\").get()\n",
    "    return {\"post_data\": post_data, \"info\": info, \"cursor\": cursor_id}\n",
    "\n",
    "\n",
    "async def scrape_subreddit(subreddit_id: str, sort: Union[\"new\", \"hot\", \"old\"], max_pages: int = None):\n",
    "    \"\"\"scrape articles on a subreddit\"\"\"\n",
    "    base_url = f\"https://www.reddit.com/r/{subreddit_id}/\"\n",
    "    response = await client.get(base_url)\n",
    "    subreddit_data = {}\n",
    "    data = parse_subreddit(response)\n",
    "    subreddit_data[\"info\"] = data[\"info\"]\n",
    "    subreddit_data[\"posts\"] = data[\"post_data\"]\n",
    "    cursor = data[\"cursor\"]\n",
    "\n",
    "    def make_pagination_url(cursor_id: str):\n",
    "        return f\"https://www.reddit.com/svc/shreddit/community-more-posts/hot/?after={cursor_id}%3D%3D&t=DAY&name={subreddit_id}&feedLength=3&sort={sort}\" \n",
    "        \n",
    "    while cursor and (max_pages is None or max_pages > 0):\n",
    "        url = make_pagination_url(cursor)\n",
    "        response = await client.get(url)\n",
    "        data = parse_subreddit(response)\n",
    "        cursor = data[\"cursor\"]\n",
    "        post_data = data[\"post_data\"]\n",
    "        subreddit_data[\"posts\"].extend(post_data)\n",
    "        if max_pages is not None:\n",
    "            max_pages -= 1\n",
    "    log.success(f\"scraped {len(subreddit_data['posts'])} posts from the rubreddit: r/{subreddit_id}\")\n",
    "    return subreddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33419a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-26 17:13:38.758\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/Entrepreneur\u001b[0m\n",
      "\u001b[32m2025-05-26 17:13:42.851\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/SaaS\u001b[0m\n",
      "\u001b[32m2025-05-26 17:13:46.310\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/NoStupidQuestions\u001b[0m\n",
      "\u001b[32m2025-05-26 17:13:51.403\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/personalfinance\u001b[0m\n",
      "\u001b[32m2025-05-26 17:13:55.155\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/smallbusiness\u001b[0m\n",
      "\u001b[32m2025-05-26 17:13:59.927\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/socialmedia\u001b[0m\n",
      "\u001b[32m2025-05-26 17:14:03.142\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/askatherapist\u001b[0m\n",
      "\u001b[32m2025-05-26 17:14:06.737\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/productivity\u001b[0m\n",
      "\u001b[32m2025-05-26 17:14:10.318\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_subreddit\u001b[0m:\u001b[36m95\u001b[0m - \u001b[32m\u001b[1mscraped 53 posts from the rubreddit: r/Accounting\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "async def run():\n",
    "    # Create the main data directory if it doesn't exist\n",
    "    os.makedirs(\"../data/\", exist_ok=True)\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        data = await scrape_subreddit(\n",
    "            subreddit_id=subreddit,\n",
    "            sort=\"new\",\n",
    "            max_pages=2\n",
    "        )\n",
    "        \n",
    "        # Create subfolder for each subreddit\n",
    "        subreddit_folder = os.path.join(\"../data/\", subreddit)\n",
    "        os.makedirs(subreddit_folder, exist_ok=True)\n",
    "        \n",
    "        # Save data in the subreddit's subfolder\n",
    "        file_path = os.path.join(subreddit_folder, \"subreddit.json\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b05c6a",
   "metadata": {},
   "source": [
    "# Scrapping comments from posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfb7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Union\n",
    "from httpx import AsyncClient, Response\n",
    "from parsel import Selector\n",
    "from loguru import logger as log\n",
    "\n",
    "client = AsyncClient(\n",
    "    # previous client configuration\n",
    ")\n",
    "\n",
    "def parse_post_info(response: Response) -> Dict:\n",
    "    \"\"\"parse post data from a subreddit post\"\"\"\n",
    "    selector = Selector(response.text)\n",
    "    info = {}\n",
    "    label = selector.xpath(\"//faceplate-tracker[@source='post']/a/span/div/text()\").get()\n",
    "    comments = selector.xpath(\"//shreddit-post/@comment-count\").get()\n",
    "    upvotes = selector.xpath(\"//shreddit-post/@score\").get()\n",
    "    info[\"authorId\"] = selector.xpath(\"//shreddit-post/@author-id\").get()\n",
    "    info[\"author\"] = selector.xpath(\"//shreddit-post/@author\").get()\n",
    "    info[\"authorProfile\"] = \"https://www.reddit.com/user/\" + info[\"author\"] if info[\"author\"] else None\n",
    "    info[\"subreddit\"] = selector.xpath(\"//shreddit-post/@subreddit-prefixed-name\").get()\n",
    "    info[\"postId\"] = selector.xpath(\"//shreddit-post/@id\").get()\n",
    "    info[\"postLabel\"] = label.strip() if label else None\n",
    "    info[\"publishingDate\"] = selector.xpath(\"//shreddit-post/@created-timestamp\").get()\n",
    "    info[\"postTitle\"] = selector.xpath(\"//shreddit-post/@post-title\").get()\n",
    "    info[\"postLink\"] = selector.xpath(\"//shreddit-canonical-url-updater/@value\").get()\n",
    "    info[\"commentCount\"] = int(comments) if comments else None\n",
    "    info[\"upvoteCount\"] = int(upvotes) if upvotes else None\n",
    "    info[\"attachmentType\"] = selector.xpath(\"//shreddit-post/@post-type\").get()\n",
    "    info[\"attachmentLink\"] = selector.xpath(\"//shreddit-post/@content-href\").get()\n",
    "    return info\n",
    "\n",
    "\n",
    "def parse_post_comments(response: Response) -> List[Dict]:\n",
    "    \"\"\"parse post comments\"\"\"\n",
    "\n",
    "    def parse_comment(parent_selector) -> Dict:\n",
    "        \"\"\"parse a comment object\"\"\"\n",
    "        author = parent_selector.xpath(\"./@data-author\").get()\n",
    "        link = parent_selector.xpath(\"./@data-permalink\").get()\n",
    "        dislikes = parent_selector.xpath(\".//span[contains(@class, 'dislikes')]/@title\").get()\n",
    "        upvotes = parent_selector.xpath(\".//span[contains(@class, 'likes')]/@title\").get()\n",
    "        downvotes = parent_selector.xpath(\".//span[contains(@class, 'unvoted')]/@title\").get()        \n",
    "        return {\n",
    "            \"authorId\": parent_selector.xpath(\"./@data-author-fullname\").get(),\n",
    "            \"author\": author,\n",
    "            \"authorProfile\": \"https://www.reddit.com/user/\" + author if author else None,\n",
    "            \"commentId\": parent_selector.xpath(\"./@data-fullname\").get(),\n",
    "            \"link\": \"https://www.reddit.com\" + link if link else None,\n",
    "            \"publishingDate\": parent_selector.xpath(\".//time/@datetime\").get(),\n",
    "            \"commentBody\": parent_selector.xpath(\".//div[@class='md']/p/text()\").get(),\n",
    "            \"upvotes\": int(upvotes) if upvotes else None,\n",
    "            \"dislikes\": int(dislikes) if dislikes else None,\n",
    "            \"downvotes\": int(downvotes) if downvotes else None,            \n",
    "        }\n",
    "\n",
    "    def parse_replies(what) -> List[Dict]:\n",
    "        \"\"\"recursively parse replies\"\"\"\n",
    "        replies = []\n",
    "        for reply_box in what.xpath(\".//div[@data-type='comment']\"):\n",
    "            reply_comment = parse_comment(reply_box)\n",
    "            child_replies = parse_replies(reply_box)\n",
    "            if child_replies:\n",
    "                reply_comment[\"replies\"] = child_replies\n",
    "            replies.append(reply_comment)\n",
    "        return replies\n",
    "\n",
    "    selector = Selector(response.text)\n",
    "    data = []\n",
    "    for item in selector.xpath(\"//div[@class='sitetable nestedlisting']/div[@data-type='comment']\"):\n",
    "        comment_data = parse_comment(item)\n",
    "        replies = parse_replies(item)\n",
    "        if replies:\n",
    "            comment_data[\"replies\"] = replies\n",
    "        data.append(comment_data)            \n",
    "    return data\n",
    "\n",
    "\n",
    "async def scrape_post(url: str, sort: Union[\"old\", \"new\", \"top\"]) -> Dict:\n",
    "    \"\"\"scrape subreddit post and comment data\"\"\"\n",
    "    response = await client.get(url)\n",
    "    post_data = {}\n",
    "    post_data[\"info\"] = parse_post_info(response)\n",
    "    # scrape the comments from the old.reddit version, with the same post URL \n",
    "    post_link = post_data[\"info\"][\"postLink\"]\n",
    "    if post_link:\n",
    "        bulk_comments_page_url = post_link.replace(\"www\", \"old\") + f\"?sort={sort}&limit=500\"\n",
    "    else:\n",
    "        # fallback to using the original URL if postLink is None\n",
    "        bulk_comments_page_url = url.replace(\"www\", \"old\") + f\"?sort={sort}&limit=500\"\n",
    "    response = await client.get(bulk_comments_page_url)\n",
    "    post_data[\"comments\"] = parse_post_comments(response) \n",
    "    log.success(f\"scraped {len(post_data['comments'])} comments from the post {url}\")\n",
    "    return post_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec1736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping comments from posts:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Entrepreneur ../data/Entrepreneur\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:01.846\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 149 comments from the post https://www.reddit.com/r/Entrepreneur/comments/1kvkyh4/whats_an_industry_that_desperately_needs_younger/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:03.828\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 64 comments from the post https://www.reddit.com/r/Entrepreneur/comments/1kuhsdu/i_keep_seeing_the_same_revenue_leak_in_every/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:05.352\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 16 comments from the post https://www.reddit.com/r/Entrepreneur/comments/1kw1ui2/what_has_been_the_biggest_win_for_your_career/\u001b[0m\n",
      "Scraping posts from r/Entrepreneur: 100%|██████████| 3/3 [00:08<00:00,  2.82s/it]\n",
      "Scraping comments from posts:  11%|█         | 1/9 [00:08<01:07,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/SaaS ../data/SaaS\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:07.436\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 62 comments from the post https://www.reddit.com/r/SaaS/comments/1kvsq47/i_removed_aipowered_from_all_my_b2b_copy/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:09.724\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 89 comments from the post https://www.reddit.com/r/SaaS/comments/1krurou/i_spent_6_months_building_an_app_that_made/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:11.239\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 19 comments from the post https://www.reddit.com/r/SaaS/comments/1kvuzkn/saas_founders_with_more_than_1m_arr_what_tools_do/\u001b[0m\n",
      "Scraping posts from r/SaaS: 100%|██████████| 3/3 [00:05<00:00,  1.96s/it]\n",
      "Scraping comments from posts:  22%|██▏       | 2/9 [00:14<00:48,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/NoStupidQuestions ../data/NoStupidQuestions\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:17.324\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 217 comments from the post https://www.reddit.com/r/NoStupidQuestions/comments/1kvv3ca/what_is_the_hotel_receptionist_doing_on_the/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:24.172\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 204 comments from the post https://www.reddit.com/r/NoStupidQuestions/comments/1kougfo/why_arent_former_american_slave_plantations/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:29.960\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 206 comments from the post https://www.reddit.com/r/NoStupidQuestions/comments/1kvvst0/why_dont_evangelical_missionaries_ever_go_to/\u001b[0m\n",
      "Scraping posts from r/NoStupidQuestions: 100%|██████████| 3/3 [00:18<00:00,  6.24s/it]\n",
      "Scraping comments from posts:  33%|███▎      | 3/9 [00:33<01:13, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/personalfinance ../data/personalfinance\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:31.254\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 10 comments from the post https://www.reddit.com/r/personalfinance/comments/1kw2bpv/got_1000mo_raise_after_taxes_diverting_it_all_in/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:34.380\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 60 comments from the post https://www.reddit.com/r/personalfinance/comments/1ksqnmx/billed_1300_for_a_free_screening_from_my_urologist/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:35.960\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 29 comments from the post https://www.reddit.com/r/personalfinance/comments/1kvudqm/how_to_continue_the_conversation_with_hr_around/\u001b[0m\n",
      "Scraping posts from r/personalfinance: 100%|██████████| 3/3 [00:05<00:00,  2.00s/it]\n",
      "Scraping comments from posts:  44%|████▍     | 4/9 [00:39<00:49,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/smallbusiness ../data/smallbusiness\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:37.031\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 15 comments from the post https://www.reddit.com/r/smallbusiness/comments/1kw1ajm/today_i_got_my_first_customer/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:41.423\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 204 comments from the post https://www.reddit.com/r/smallbusiness/comments/1kqo3ts/anybody_ever_question_why_tf_were_still_doing_this/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:44.125\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 57 comments from the post https://www.reddit.com/r/smallbusiness/comments/1kvumu0/those_who_sold_what_do_you_do_now/\u001b[0m\n",
      "Scraping posts from r/smallbusiness: 100%|██████████| 3/3 [00:08<00:00,  2.72s/it]\n",
      "Scraping comments from posts:  56%|█████▌    | 5/9 [00:47<00:36,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/socialmedia ../data/socialmedia\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:45.999\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 1 comments from the post https://www.reddit.com/r/socialmedia/comments/1kw14i6/whats_your_post_and_pray_metric_that_actually/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:48.384\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 78 comments from the post https://www.reddit.com/r/socialmedia/comments/1ktkqa3/are_millennials_done_with_social_media_or_just/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:49.813\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 6 comments from the post https://www.reddit.com/r/socialmedia/comments/1kvpcl5/is_it_better_to_post_daily_or_focus_on/\u001b[0m\n",
      "Scraping posts from r/socialmedia: 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]\n",
      "Scraping comments from posts:  67%|██████▋   | 6/9 [00:52<00:24,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/askatherapist ../data/askatherapist\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:51.039\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 14 comments from the post https://www.reddit.com/r/askatherapist/comments/1kvy7ve/is_longterm_therapy_always_the_clients_decision/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:52.558\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 17 comments from the post https://www.reddit.com/r/askatherapist/comments/1koccc5/is_it_weird_that_im_fascinated_by_very_basic/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:53.716\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 6 comments from the post https://www.reddit.com/r/askatherapist/comments/1kvy08u/can_i_still_see_my_therapist_when_i_move_for/\u001b[0m\n",
      "Scraping posts from r/askatherapist: 100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n",
      "Scraping comments from posts:  78%|███████▊  | 7/9 [00:56<00:13,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/productivity ../data/productivity\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:54.784\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 11 comments from the post https://www.reddit.com/r/productivity/comments/1kvw3iq/whats_something_that_used_to_stress_you_out/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:56.334\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 50 comments from the post https://www.reddit.com/r/productivity/comments/1krinsq/fake_commuting_helps_me_work/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:15:57.770\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 31 comments from the post https://www.reddit.com/r/productivity/comments/1kvwgmn/whats_the_one_productivity_hack_everyone_talks/\u001b[0m\n",
      "Scraping posts from r/productivity: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]\n",
      "Scraping comments from posts:  89%|████████▉ | 8/9 [01:00<00:05,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Accounting ../data/Accounting\\subreddit.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-27 12:15:59.608\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 47 comments from the post https://www.reddit.com/r/Accounting/comments/1kw3drg/thank_you_accounting/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:16:04.212\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 137 comments from the post https://www.reddit.com/r/Accounting/comments/1kamidy/antiwfh_people_are_the_laziest_employees_weve_got/\u001b[0m\n",
      "\u001b[32m2025-05-27 12:16:05.845\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrape_post\u001b[0m:\u001b[36m94\u001b[0m - \u001b[32m\u001b[1mscraped 24 comments from the post https://www.reddit.com/r/Accounting/comments/1kvvhm6/i_enjoyed_business_school_but_never_want_to_work/\u001b[0m\n",
      "Scraping posts from r/Accounting: 100%|██████████| 3/3 [00:08<00:00,  2.69s/it]\n",
      "Scraping comments from posts: 100%|██████████| 9/9 [01:08<00:00,  7.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "async def run():\n",
    "    comments_and_posts = []\n",
    "    for subreddit in tqdm(subreddits, desc=\"Scraping comments from posts\"):\n",
    "        \n",
    "        subreddit_folder = os.path.join(\"../data/\", subreddit)\n",
    "        file_path = os.path.join(subreddit_folder, \"subreddit.json\")\n",
    "        \n",
    "        print(subreddit_folder, file_path)\n",
    "        subreddit_data = json.load(open(file_path, \"r\", encoding=\"utf-8\"))\n",
    "        \n",
    "        for post in tqdm(\n",
    "            subreddit_data[\"posts\"][:3],\n",
    "            desc=f\"Scraping posts from r/{subreddit}\"\n",
    "        ):\n",
    "            comments_and_posts.append(post[\"title\"])\n",
    "            \n",
    "            post_url = post[\"link\"]\n",
    "            post_data = await scrape_post(url=post_url, sort=\"top\")\n",
    "            \n",
    "            comments_from_post = post_data[\"comments\"]\n",
    "            # Vamos ignorar replies\n",
    "            for comment in comments_from_post:\n",
    "                comments_and_posts.append(comment[\"commentBody\"])\n",
    "                \n",
    "    # save the comments and posts to a file\n",
    "    with open(\"../data/comments/comments_and_posts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(comments_and_posts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f6a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
